---
title: SIFT Moves  
layout: single  
permalink: /sift/  
author_profile: true
toc: true
toc_sticky: true
---

By [Ryan Randall](https://orcid.org/0000-0003-4499-7255)  

## Don't Stress, SIFT  

Evaluating information can be exhausting. Even before social media or the internet, library collections often caused information overload. And with new social media sites popping up routinely, it seems like there's always another platform where people can share their own, unverified content.  

Before throwing your hands up in despair at how unruly our modern information environments can be, try practicing the strategies from this section. They'll help you find better, more reliable sources—and they're great to share with patrons and colleagues. They're adaptable strategies that apply to online and offline information.  

This section also has some very specific tools and practices for online info. Unfortunately, there isn't a quick & simple test that can guarantee an answer about whether a site is trustworthy or a source is worth more of your time. But there are many strategies and approaches that can profoundly increase your chances of finding reliable information and avoiding mistaken or intentionally deceptive junk.  

In much the same way that "don't hit any other cars" isn't particularly useful driving advice, "don't read anything sketchy" sounds useful without being helpful. When you're learning to drive, it's way better to have someone remind you to always look over your shoulder into the next lane before actually turning the wheel. Once this small, quick habit becomes second nature, you're well on your way to being a reliably safer driver. Thankfully, there are similarly small moves we can apply to information literacy.  

"SIFT" helps you remember some of these strategies.   

## SIFT Details  

Click on each letter below for details about that part of the acronym.  

<details markdown="1">
  <summary>S: Stop</summary>  
  <div markdown="1" class="details-wrapper">
  It sounds counterintuitive, but this model suggests that when you first find a source, you **don't** start by reading it. Why waste even a couple minutes reading garbage when you could just avoid engaging to begin with? So instead of jumping right into the content of what you find, try to place it in context.  

  Ask yourself, "What's the platform or publisher behind this article, book, blog post, etc.? Who wrote this work? What's the reputation of the site, publisher, or social media network?" If you don't already know those details, don't engage with it until you've found the answers.  

  Returning to our "learning to drive" analogy, you've probably developed the habit of looking over your shoulder before changing lanes. You know that shouldn't impulsively act—you first need to be aware of what is over there. Awareness of context goes for what you read as well, even if the consequences aren't as immediate.  

  This basic behavior of gaining context first goes not just for a single source, but also for when using search engines. Instead of clicking right away on the top result, spend 10 or 15 seconds scanning the results. You'll often see titles, source descriptions, section headings that group results into particular categories, and other contextual cues, all right there.  

  Stanford's History Education Group calls this short pause "click restraint" when they discuss lateral reading (sometimes called horizontal reading). Lateral reading one of the best ways to do the next move: investigating the source.  
  </div>
</details>

<details markdown="1">
  <summary>I: Investigate the Source</summary>  
  <div markdown="1" class="details-wrapper">
  So how do we establish context about a source? How can you tell whether the article you're reading about the health properties of sugar beets is written by a well-known nutritionist or is actually "sponsored" (i.e. paid-to-place) content written by the advertising wing of a sugar beet company?  

  One of the quickest and most reliable ways is to practice **click restraint**. Don't even immediately click on the "About Page"! Although that page could be helpful, it could also be deceptive. The best approach is [lateral reading](https://cor.stanford.edu/curriculum/collections/teaching-lateral-reading)—leaving a site to see what other sources say about it.  
  
  Instead of taking the source's own word as reliable, search for its author, publication, or site on Wikipedia. Wikipedia has downsides, of course. But since it's crowdsourced, its contents are continually being corrected, expanded, and otherwise improved. It's a quick way to check whether a person or publication is considered notable enough for an entry.  

  This "notability" decision can be tricky, unfortunately. Wikipedia is often critiqued for having biases against creating standalone entries for people who are minorities, women, and/or professors. It also is far less likely to have entries for rural authors and subjects. So if there is an entry, the contents of that entry can help you evaluate the source more quickly. But if there isn't an entry, you'll probably want to think about why that might be instead of immediately deciding that the subject isn't worthwhile at all.  

  If there is an entry for the person, publication, or other source, you don't have to read the whole thing. You can skim the first section, then the titles of any other sections, looking for praise and awards as well as for controversies or red flags. Using your web browser's built-in "find on this page" tool can make this skimming even faster.  
  </div>
</details>

<details markdown="1">
  <summary>F: Find Better Coverage</summary>  
  <div markdown="1" class="details-wrapper">
  When you're searching, you often care most about the claim. If you're curious about potential health effects of using sugar from sugar beets, you're probably interested in the health effects, not the reputation of any particular author or publication. In that case, the fastest thing is probably to try to find whether experts have reached agreement on this, some sort of consensus viewpoint, or if experts still consider this as up for debate, a subject around which there's still a great deal of emerging knowledge.  

  You don't have to adopt that viewpoint yourself, of course. But it's helpful to know whether a claim is seen as true only by a tiny group or if it's the stance of an entire profession.  
  </div>
</details>

<details markdown="1">
  <summary>T: Trace Claims, Quotes, and Media Back to the Original Context</summary>  
  <div markdown="1" class="details-wrapper">
  One of the easiest ways to convincingly mislead people is to take a photo, video, quote, or claim and circulate it out of context. Have you ever played the game of "telephone," where someone whispers a sentence into another person's ear, then they whisper it into the next person's ear, etc.? With each repetition, new misunderstandings get added, until the last person in the chain usually hears something totally unrelated to the original.  

  For photos, [Tin Eye](https://tineye.com/) lets you quickly upload or link to an image and see where it originally came from. It has a drop-down menu that let you see the oldest image matches at the top of the list. So if you see someone claiming that there's a herd of goats on the loose in a suburban neighborhood near you—and includes an image—you can upload that image and find out that it was actually taken in [Boise back in 2018](https://tineye.com/search/57083cdccaa9c1fa1eebec1711ec36e4e1c8b1b1?sort=crawl_date&order=asc&page=1).  
  </div>
</details>

## Additional Useful Approaches  

SIFT highly suggests looking beyond the initial source. Instead of getting overwhelmed by the difficulties of information evaluation, you can use the abundance of information sources to your advantage. Don't waste time on just any glittery, sparkly thing—keep sifting through sources until you find something that seems likely to be a real gem.  

How will you know it's a gem? Of course, there are strategies for evaluating things in further depth as well. Stanford's [Civic Online Reasoning](https://cor.stanford.edu/) is one of the quickest. It asks: 
1. Who's behind the information?  
2. What's the evidence?  
3. What do other sources say?  

You've probably already noticed that point 1 sounds a lot like "investigate the source" and point 3 sounds a lot like "find better coverage." That part in the middle—"what's the evidence?"—can take a very long time to decipher, depending on the source.  

Sources use a wide range of types of evidence. A scientific source explaining how magnets work would use very different types of evidence than a humanities source explaining why video games can be a modern form of art! Even if you're just talking about evidence around "who, when, what, where, how, and why", journalistic evidence and historical evidence would approach those questions in different ways. The better we get at recognizing different types of information, the better we get at learning to evaluate them.  

This also goes for recognizing the goals, or purpose, each source has for sharing information. If we realize that we're engaging with a satirical source like the Onion, the Daily Show, or even an old George Carlin stand-up recording, we'll realize that their purpose is to entertain us or to see something in a new light. This isn't a difficulty for the internet alone. As embarrassing as it is, I can still remember becoming truly angry when I read Jonathan Swift's [A Modest Proposal](https://gitenberg.org/book/1080) in high school, since I didn't stop to look at the top of the page in our textbook. The running chapter heading would have quickly told me that this was the first reading in our new chapter: satire.  

One common model for evaluating sources uses the acronym "CRAAP." It aims to reminds you to consider whether a source is current, reliable, accurate, authoritative, and pertinent. These are all important facets to consider when selecting a source. But [research increasingly shows](https://www.ala.org/acrl/sites/ala.org.acrl/files/content/conferences/confsandpreconfs/2021/StudentAchievementUnlocked.pdf) that students taught to evaluate using this model don't use it reliably. <!-- do more about why it's not a test, also link to Stanford COR, Caulfield, and other sources on CRAAP --> 

<!-- Add New Literacy's question about "what is this source trying to do," and maybe introduce the idea of source genres / genre literacy -->

## Attribution  

In putting this section together, I (Ryan) have drawn from a range of sources. Attribution and appreciation to:  

- Mike Caulfield's resources  
  - [SIFT (the Four Moves)](https://hapgood.us/2019/06/19/sift-the-four-moves/)  
  - [Check, Please! Starter Course](https://www.notion.so/Check-Please-Starter-Course-ae34d043575e42828dc2964437ea4eed)  
  - [Web Literacy for Student Fact Checkers](https://webliteracy.pressbooks.com)  
  - [Sifting through the Coronavirus Pandemic](https://infodemic.blog)  
  - [A Short History of CRAAP](https://hapgood.us/2018/09/14/a-short-history-of-craap/)  
- Andrea Baer and Dan Kipnis's [Evaluating Online Sources: A Toolkit](https://libguides.rowan.edu/EvaluatingOnlineSources)  
- Stanford's [Civic Online Reasoning](https://cor.stanford.edu/) curriculum  
- Stonybrook's [Center for News Literacy](http://www.centerfornewsliteracy.org/)  
